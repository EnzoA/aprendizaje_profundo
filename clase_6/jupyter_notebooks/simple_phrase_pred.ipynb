{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"colab":{"name":"simple_phrase_pred.ipynb","provenance":[]},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"AFR3pz3JiGlE"},"source":["# simple_phrase_predictor\n","## fuente: https://blog.floydhub.com/a-beginners-guide-on-recurrent-neural-networks-with-pytorch/\n","## codigo tomado de: https://github.com/gabrielloye/RNN-walkthrough"]},{"cell_type":"markdown","metadata":{"id":"qUXUHhpeiFIU"},"source":["Welcome to this notebook where we'll be implementing a simple RNN character model with PyTorch to familiarize ourselves with the PyTorch library and get started with RNNs. You can run the code weâ€™re using on FloydHub by clicking the button below and creating the project as well.\n","\n","[![Run on FloydHub](https://static.floydhub.com/button/button-small.svg)](https://floydhub.com/run?template=https://github.com/gabrielloye/RNN-walkthrough)"]},{"cell_type":"markdown","metadata":{"id":"QEv9uj8SiFJK"},"source":["In this implementation, we'll be building a model that can complete your sentence based on a few characters or a word used as input.\n","![Example](img/Slide4.jpg)\n","To keep this short and simple, we won't be using any large or external datasets. Instead, we'll just be defining a few sentences to see how the model learns from these sentences. The process that this implementation will take is as follows:\n","![Overview](img/Slide5.jpg)"]},{"cell_type":"markdown","metadata":{"id":"fX5jH-eCiFJP"},"source":["We'll start off by importing the main PyTorch package along with the *Variable* class used to store our data tensors and the *nn* package which we will use when building the model. In addition, we'll only be using numpy to pre-process our data as Torch works really well with numpy."]},{"cell_type":"code","metadata":{"id":"56pg1YydiFJV","executionInfo":{"status":"ok","timestamp":1637888243404,"user_tz":180,"elapsed":5376,"user":{"displayName":"Marcos Uriel Maillot","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16876029369473785241"}}},"source":["import torch\n","from torch import nn\n","\n","import numpy as np"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uA38Tv3niFJa"},"source":["First, we'll define the sentences that we want our model to output when fed with the first word or the first few characters.\n","\n","Then we'll create a dictionary out of all the characters that we have in the sentences and map them to an integer. This will allow us to convert our input characters to their respective integers (*char2int*) and vice versa (*int2char*)."]},{"cell_type":"code","metadata":{"id":"aKMPzcuPiFJc","executionInfo":{"status":"ok","timestamp":1637888243406,"user_tz":180,"elapsed":79,"user":{"displayName":"Marcos Uriel Maillot","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16876029369473785241"}}},"source":["#text = ['hey how are you','good i am fine','have a nice day']\n","\n","#text = ['hey how are you','good i am fine','have a nice day', 'its someone there', 'may the force be with you', 'dont tell me the odds', 'iam your father', 'the force will be with you always', 'do or do not there is no try']\n","\n","text = ['hola que tal', 'como te llamas?', 'queres comer algo?', 'me llamo marcos', 'este mensaje fue adivinado',\n","        'este mensajo esta mal escrito', 'tengo que completar algo', 'buenos dias', 'feliz domingo', 'al final todo se resuelve',\n","        'otro mensaje mas', 'espero que funcione', 'siempre que llovio paro', 'pajaro que comio volo', 'el habito no hace al monje',\n","        'la vida sin musica seria un error']\n","# Join all the sentences together and extract the unique characters from the combined sentences\n","chars = set(''.join(text))\n","\n","# Creating a dictionary that maps integers to the characters\n","int2char = dict(enumerate(chars))\n","\n","# Creating another dictionary that maps characters to integers\n","char2int = {char: ind for ind, char in int2char.items()}"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IsK9tz9biFJe","executionInfo":{"status":"ok","timestamp":1637888243410,"user_tz":180,"elapsed":75,"user":{"displayName":"Marcos Uriel Maillot","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16876029369473785241"}},"outputId":"b13cdb8b-58fe-4835-8c0e-ea6c759f4f25"},"source":["print(char2int)"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["{'h': 0, 'u': 1, 'd': 2, 'a': 3, ' ': 4, 'i': 5, 'c': 6, 'p': 7, '?': 8, 'o': 9, 'b': 10, 'm': 11, 'n': 12, 'f': 13, 'r': 14, 'g': 15, 'j': 16, 's': 17, 'z': 18, 'e': 19, 'v': 20, 't': 21, 'q': 22, 'l': 23}\n"]}]},{"cell_type":"markdown","metadata":{"id":"z6sWu9btiFJl"},"source":["Next, we'll be padding our input sentences to ensure that all the sentences are of the sample length. While RNNs are typically able to take in variably sized inputs, we will usually want to feed training data in batches to speed up the training process. In order to used batches to train on our data, we'll need to ensure that each sequence within the input data are of equal size.\n","\n","Therefore, in most cases, padding can be done by filling up sequences that are too short with **0** values and trimming sequences that are too long. In our case, we'll be finding the length of the longest sequence and padding the rest of the sentences with blank spaces to match that length."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-qE-1WyeiFJn","executionInfo":{"status":"ok","timestamp":1637888243434,"user_tz":180,"elapsed":90,"user":{"displayName":"Marcos Uriel Maillot","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16876029369473785241"}},"outputId":"3253ebb5-8c6e-4e97-9259-5a79bab84de0"},"source":["maxlen = len(max(text, key=len))\n","print(\"The longest string has {} characters\".format(maxlen))"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["The longest string has 33 characters\n"]}]},{"cell_type":"code","metadata":{"id":"r1CYZ2Z_iFJr","executionInfo":{"status":"ok","timestamp":1637888243436,"user_tz":180,"elapsed":84,"user":{"displayName":"Marcos Uriel Maillot","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16876029369473785241"}}},"source":["# Padding\n","\n","# A simple loop that loops through the list of sentences and adds a ' ' whitespace until the length of the sentence matches\n","# the length of the longest sentence\n","for i in range(len(text)):\n","    while len(text[i])<maxlen:\n","        text[i] += ' '"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iF_z94QriFJt"},"source":["As we're going to predict the next character in the sequence at each time step, we'll have to divide each sentence into\n","\n","- Input data\n","    - The last input character should be excluded as it does not need to be fed into the model\n","- Target/Ground Truth Label\n","    - One time-step ahead of the Input data as this will be the \"correct answer\" for the model at each time step corresponding to the input data"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x_YTweRfiFJv","executionInfo":{"status":"ok","timestamp":1637888243440,"user_tz":180,"elapsed":87,"user":{"displayName":"Marcos Uriel Maillot","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16876029369473785241"}},"outputId":"fbed624e-70d5-476b-8784-344febbe6daa"},"source":["# Creating lists that will hold our input and target sequences\n","input_seq = []\n","target_seq = []\n","\n","for i in range(len(text)):\n","    # Remove last character for input sequence\n","    input_seq.append(text[i][:-1])\n","    \n","    # Remove firsts character for target sequence\n","    target_seq.append(text[i][1:])\n","    print(\"Input Sequence: {}\\nTarget Sequence: {}\".format(input_seq[i], target_seq[i]))"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Input Sequence: hola que tal                    \n","Target Sequence: ola que tal                     \n","Input Sequence: como te llamas?                 \n","Target Sequence: omo te llamas?                  \n","Input Sequence: queres comer algo?              \n","Target Sequence: ueres comer algo?               \n","Input Sequence: me llamo marcos                 \n","Target Sequence: e llamo marcos                  \n","Input Sequence: este mensaje fue adivinado      \n","Target Sequence: ste mensaje fue adivinado       \n","Input Sequence: este mensajo esta mal escrito   \n","Target Sequence: ste mensajo esta mal escrito    \n","Input Sequence: tengo que completar algo        \n","Target Sequence: engo que completar algo         \n","Input Sequence: buenos dias                     \n","Target Sequence: uenos dias                      \n","Input Sequence: feliz domingo                   \n","Target Sequence: eliz domingo                    \n","Input Sequence: al final todo se resuelve       \n","Target Sequence: l final todo se resuelve        \n","Input Sequence: otro mensaje mas                \n","Target Sequence: tro mensaje mas                 \n","Input Sequence: espero que funcione             \n","Target Sequence: spero que funcione              \n","Input Sequence: siempre que llovio paro         \n","Target Sequence: iempre que llovio paro          \n","Input Sequence: pajaro que comio volo           \n","Target Sequence: ajaro que comio volo            \n","Input Sequence: el habito no hace al monje      \n","Target Sequence: l habito no hace al monje       \n","Input Sequence: la vida sin musica seria un erro\n","Target Sequence: a vida sin musica seria un error\n"]}]},{"cell_type":"markdown","metadata":{"id":"b_lYp50WiFJx"},"source":["Now we can convert our input and target sequences to sequences of integers instead of characters by mapping them using the dictionaries we created above. This will allow us to one-hot-encode our input sequence subsequently."]},{"cell_type":"code","metadata":{"id":"FsdlmgVOiFJ0","executionInfo":{"status":"ok","timestamp":1637888243443,"user_tz":180,"elapsed":83,"user":{"displayName":"Marcos Uriel Maillot","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16876029369473785241"}}},"source":["for i in range(len(text)):\n","    input_seq[i] = [char2int[character] for character in input_seq[i]]\n","    target_seq[i] = [char2int[character] for character in target_seq[i]]"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KRFTaBxAiFJ2"},"source":["Before encoding our input sequence into one-hot vectors, we'll define 3 key variables:\n","\n","- *dict_size*: The number of unique characters that we have in our text\n","    - This will determine the one-hot vector size as each character will have an assigned index in that vector\n","- *seq_len*: The length of the sequences that we're feeding into the model\n","    - As we standardised the length of all our sentences to be equal to the longest sentences, this value will be the max length - 1 as we removed the last character input as well\n","- *batch_size*: The number of sentences that we defined and are going to feed into the model as a batch"]},{"cell_type":"code","metadata":{"id":"y8pjMiN5iFJ3","executionInfo":{"status":"ok","timestamp":1637888243445,"user_tz":180,"elapsed":83,"user":{"displayName":"Marcos Uriel Maillot","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16876029369473785241"}}},"source":["dict_size = len(char2int)\n","seq_len = maxlen - 1\n","batch_size = len(text)\n","\n","def one_hot_encode(sequence, dict_size, seq_len, batch_size):\n","    # Creating a multi-dimensional array of zeros with the desired output shape\n","    features = np.zeros((batch_size, seq_len, dict_size), dtype=np.float32)\n","    \n","    # Replacing the 0 at the relevant character index with a 1 to represent that character\n","    for i in range(batch_size):\n","        for u in range(seq_len):\n","            features[i, u, sequence[i][u]] = 1\n","    return features"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l2Ei4qwxiFJ5"},"source":["We also defined a helper function that creates arrays of zeros for each character and replaces the corresponding character index with a **1**."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FJ_8aSuliFJ7","executionInfo":{"status":"ok","timestamp":1637888243447,"user_tz":180,"elapsed":83,"user":{"displayName":"Marcos Uriel Maillot","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16876029369473785241"}},"outputId":"22165024-c840-4019-e3bd-7feb5088ff96"},"source":["input_seq = one_hot_encode(input_seq, dict_size, seq_len, batch_size)\n","print(\"Input shape: {} --> (Batch Size, Sequence Length, One-Hot Encoding Size)\".format(input_seq.shape))"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Input shape: (16, 32, 24) --> (Batch Size, Sequence Length, One-Hot Encoding Size)\n"]}]},{"cell_type":"markdown","metadata":{"id":"zeO7exktiFKC"},"source":["Since we're done with all the data pre-processing, we can now move the data from numpy arrays to PyTorch's very own data structure - **Torch Tensors**"]},{"cell_type":"code","metadata":{"id":"Nz_Sy7b8iFKE","executionInfo":{"status":"ok","timestamp":1637888243448,"user_tz":180,"elapsed":77,"user":{"displayName":"Marcos Uriel Maillot","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16876029369473785241"}}},"source":["input_seq = torch.from_numpy(input_seq)\n","target_seq = torch.Tensor(target_seq)"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AQltd4M7iFKF"},"source":["Now we've reached the fun part of this project! We'll be defining the model using the Torch library, and this is where you can add or remove layers, be it fully connected layers, convolutational layers, vanilla RNN layers, LSTM layers, and many more! In this post, we'll be using the basic nn.rnn to demonstrate a simple example of how RNNs can be used.\n","\n","Before we start building the model, let's use a build in feature in PyTorch to check the device we're running on (CPU or GPU). This implementation will not require GPU as the training is really simple. However, as you progress on to large datasets and models with millions of trainable parameters, using the GPU will be very important to speed up your training."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8Hgc_qeliFKO","executionInfo":{"status":"ok","timestamp":1637888243449,"user_tz":180,"elapsed":76,"user":{"displayName":"Marcos Uriel Maillot","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16876029369473785241"}},"outputId":"86f0a4c5-b225-49ff-97f1-f43069a55807"},"source":["# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n","is_cuda = torch.cuda.is_available()\n","\n","# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n","if is_cuda:\n","    device = torch.device(\"cuda\")\n","    print(\"GPU is available\")\n","else:\n","    device = torch.device(\"cpu\")\n","    print(\"GPU not available, CPU used\")"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["GPU not available, CPU used\n"]}]},{"cell_type":"markdown","metadata":{"id":"yJu-BlPliFKP"},"source":["To start building our own neural network model, we can define a class that inherits PyTorchâ€™s base class (nn.module) for all neural network modules. After doing so, we can start defining some variables and also the layers for our model under the constructor. For this model, weâ€™ll only be using 1 layer of RNN followed by a fully connected layer. The fully connected layer will be in-charge of converting the RNN output to our desired output shape.\n","\n","Weâ€™ll also have to define the forward pass function under forward() as a class method. The order the forward function is sequentially executed, therefore weâ€™ll have to pass the inputs and the zero-initialized hidden state through the RNN layer first, before passing the RNN outputs to the fully-connected layer. Note that we are using the layers that we defined in the constructor.\n","\n","The last method that we have to define is the method that we called earlier to initialize the hidden state - init_hidden(). This basically creates a tensor of zeros in the shape of our hidden states."]},{"cell_type":"code","metadata":{"id":"WuUpllt2iFKS","executionInfo":{"status":"ok","timestamp":1637888243451,"user_tz":180,"elapsed":71,"user":{"displayName":"Marcos Uriel Maillot","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16876029369473785241"}}},"source":["class Model_rnn(nn.Module):\n","    def __init__(self, input_size, output_size, hidden_dim, n_layers):\n","        super(Model_rnn, self).__init__()\n","\n","        # Defining some parameters\n","        self.hidden_dim = hidden_dim\n","        self.n_layers = n_layers\n","\n","        #Defining the layers\n","        # RNN Layer\n","        self.rnn = nn.RNN(input_size, hidden_dim, n_layers, batch_first=True)   \n","        # Fully connected layer\n","        self.fc = nn.Linear(hidden_dim, output_size)\n","    \n","    def forward(self, x):\n","        \n","        batch_size = x.size(0)\n","\n","        #Initializing hidden state for first input using method defined below\n","        hidden = self.init_hidden(batch_size)\n","\n","        # Passing in the input and hidden state into the model and obtaining outputs\n","        out, hidden = self.rnn(x, hidden)\n","        #print('aca es la out1: ', out.shape)\n","        # Reshaping the outputs such that it can be fit into the fully connected layer\n","        out = out.contiguous().view(-1, self.hidden_dim)\n","        #print('aca es la out: ', out.shape)\n","        #input()\n","        out = self.fc(out)\n","        \n","        return out, hidden\n","    \n","    def init_hidden(self, batch_size):\n","        # This method generates the first hidden state of zeros which we'll use in the forward pass\n","        self.hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device)\n","         # We'll send the tensor holding the hidden state to the device we specified earlier as well\n","        return self.hidden"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"g1nmff4xCnNX","executionInfo":{"status":"ok","timestamp":1637888243452,"user_tz":180,"elapsed":71,"user":{"displayName":"Marcos Uriel Maillot","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16876029369473785241"}}},"source":["class Model_LSTM(nn.Module):\n","    def __init__(self, input_size, output_size, hidden_dim, n_layers):\n","        super(Model_LSTM, self).__init__()\n","\n","        # Defining some parameters\n","        self.hidden_dim = hidden_dim\n","        self.n_layers = n_layers\n","\n","        \n","        # add an LSTM layer:\n","        self.lstm = nn.LSTM(input_size, hidden_dim, n_layers, batch_first=True )\n","        \n","        # add a fully-connected layer:\n","        self.linear = nn.Linear(hidden_dim, output_size)\n","        \n","        # initializing h0 and c0:\n","        #self.hidden = (torch.zeros(self.n_layers, batch_size, self.hidden_dim),\n","         #              torch.zeros(self.n_layers, batch_size, self.hidden_dim))\n","\n","\n","    def forward(self,seq):\n","        batch_size = seq.size(0)\n","        #Initializing hidden state for first input using method defined below\n","        hidden = self.init_hidden(batch_size)\n","        \n","        lstm_out, hidden = self.lstm(seq, self.hidden)\n","        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n","        pred = self.linear(lstm_out)\n","        return pred, hidden\n","\n","    def init_hidden(self, batch_size):\n","        # This method generates the first hidden state of zeros which we'll use in the forward pass\n","        self.hidden = (torch.zeros(self.n_layers, batch_size, self.hidden_dim),\n","                       torch.zeros(self.n_layers, batch_size, self.hidden_dim))\n","         # We'll send the tensor holding the hidden state to the device we specified earlier as well\n","        return self.hidden"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3ZeMv09MiFKT"},"source":["After defining the model above, we'll have to instantiate the model with the relevant parameters and define our hyperparamters as well. The hyperparameters we're defining below are:\n","\n","- *n_epochs*: Number of Epochs --> This refers to the number of times our model will go through the entire training dataset\n","- *lr*: Learning Rate --> This affects the rate at which our model updates the weights in the cells each time backpropogation is done\n","    - A smaller learning rate means that the model changes the values of the weight with a smaller magnitude\n","    - A larger learning rate means that the weights are updated to a larger extent for each time step\n","\n","Similar to other neural networks, we have to define the optimizer and loss function as well. Weâ€™ll be using CrossEntropyLoss as the final output is basically a classification task."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"somPZ-7jiFKU","executionInfo":{"status":"ok","timestamp":1637888295612,"user_tz":180,"elapsed":391,"user":{"displayName":"Marcos Uriel Maillot","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16876029369473785241"}},"outputId":"e5af4051-106b-4463-adef-2fb86f904513"},"source":["# Instantiate the model with hyperparameters\n","#model = Model_rnn(input_size=dict_size, output_size=dict_size, hidden_dim=13, n_layers=2)\n","\n","# Instantiate the model with hyperparameters\n","model = Model_LSTM(input_size=dict_size, output_size=dict_size, hidden_dim=200, n_layers=2)\n","\n","print(model)\n","# We'll also set the model to the device that we defined earlier (default is CPU)\n","model = model.to(device)\n","\n","# Define hyperparameters\n","n_epochs = 500\n","lr=0.01\n","\n","# Define Loss, Optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=lr)"],"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Model_LSTM(\n","  (lstm): LSTM(24, 200, num_layers=2, batch_first=True)\n","  (linear): Linear(in_features=200, out_features=24, bias=True)\n",")\n"]}]},{"cell_type":"markdown","metadata":{"id":"u_8uq4KiiFKV"},"source":["Now we can begin our training! As we only have a few sentences, this training process is very fast. However, as we progress, larger datasets and deeper models mean that the input data is much larger and the number of parameters within the model that we have to compute is much more."]},{"cell_type":"code","metadata":{"scrolled":true,"id":"0PB5LQtLiFKX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637888330060,"user_tz":180,"elapsed":30606,"user":{"displayName":"Marcos Uriel Maillot","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16876029369473785241"}},"outputId":"65542f8f-8839-4ce4-8578-8e44ba6f5920"},"source":["# Training Run\n","input_seq = input_seq.to(device)\n","for epoch in range(1, n_epochs + 1):\n","    optimizer.zero_grad() # Clears existing gradients from previous epoch\n","    #input_seq = input_seq.to(device)\n","    output, hidden = model(input_seq)\n","    output = output.to(device)\n","    target_seq = target_seq.to(device)\n","    loss = criterion(output, target_seq.view(-1).long())\n","    loss.backward() # Does backpropagation and calculates gradients\n","    optimizer.step() # Updates the weights accordingly\n","    \n","    if epoch%10 == 0:\n","        print('Epoch: {}/{}.............'.format(epoch, n_epochs), end=' ')\n","        print(\"Loss: {:.4f}\".format(loss.item()))"],"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 10/500............. Loss: 2.1255\n","Epoch: 20/500............. Loss: 2.3836\n","Epoch: 30/500............. Loss: 2.0206\n","Epoch: 40/500............. Loss: 2.0117\n","Epoch: 50/500............. Loss: 1.8140\n","Epoch: 60/500............. Loss: 1.6110\n","Epoch: 70/500............. Loss: 1.4244\n","Epoch: 80/500............. Loss: 1.3326\n","Epoch: 90/500............. Loss: 1.1128\n","Epoch: 100/500............. Loss: 0.8828\n","Epoch: 110/500............. Loss: 1.1779\n","Epoch: 120/500............. Loss: 0.7577\n","Epoch: 130/500............. Loss: 0.5031\n","Epoch: 140/500............. Loss: 0.3035\n","Epoch: 150/500............. Loss: 0.1820\n","Epoch: 160/500............. Loss: 0.1136\n","Epoch: 170/500............. Loss: 0.0791\n","Epoch: 180/500............. Loss: 0.0591\n","Epoch: 190/500............. Loss: 0.0462\n","Epoch: 200/500............. Loss: 0.0374\n","Epoch: 210/500............. Loss: 0.0313\n","Epoch: 220/500............. Loss: 0.0272\n","Epoch: 230/500............. Loss: 0.0243\n","Epoch: 240/500............. Loss: 0.0223\n","Epoch: 250/500............. Loss: 0.0208\n","Epoch: 260/500............. Loss: 0.0196\n","Epoch: 270/500............. Loss: 0.0187\n","Epoch: 280/500............. Loss: 0.0179\n","Epoch: 290/500............. Loss: 0.0171\n","Epoch: 300/500............. Loss: 0.0164\n","Epoch: 310/500............. Loss: 0.0157\n","Epoch: 320/500............. Loss: 0.0150\n","Epoch: 330/500............. Loss: 0.0144\n","Epoch: 340/500............. Loss: 0.0140\n","Epoch: 350/500............. Loss: 0.0137\n","Epoch: 360/500............. Loss: 0.0134\n","Epoch: 370/500............. Loss: 0.0132\n","Epoch: 380/500............. Loss: 0.0131\n","Epoch: 390/500............. Loss: 0.0129\n","Epoch: 400/500............. Loss: 0.0128\n","Epoch: 410/500............. Loss: 0.0126\n","Epoch: 420/500............. Loss: 0.0125\n","Epoch: 430/500............. Loss: 0.0124\n","Epoch: 440/500............. Loss: 0.0124\n","Epoch: 450/500............. Loss: 0.0123\n","Epoch: 460/500............. Loss: 0.0122\n","Epoch: 470/500............. Loss: 0.0121\n","Epoch: 480/500............. Loss: 0.0121\n","Epoch: 490/500............. Loss: 0.0120\n","Epoch: 500/500............. Loss: 0.0120\n"]}]},{"cell_type":"markdown","metadata":{"id":"_ES7-uUdiFKY"},"source":["Letâ€™s test our model now and see what kind of output we will get. Before that, letâ€™s define some helper function to convert our model output back to text."]},{"cell_type":"code","metadata":{"id":"6N6GU5FwiFKa","executionInfo":{"status":"ok","timestamp":1637888250145,"user_tz":180,"elapsed":64,"user":{"displayName":"Marcos Uriel Maillot","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16876029369473785241"}}},"source":["def predict(model, character):\n","    # One-hot encoding our input to fit into the model\n","    character = np.array([[char2int[c] for c in character]])\n","    character = one_hot_encode(character, dict_size, character.shape[1], 1)\n","    character = torch.from_numpy(character)\n","    character = character.to(device)\n","    \n","    out, hidden = model(character)\n","\n","    prob = nn.functional.softmax(out[-1], dim=0).data\n","    # Taking the class with the highest probability score from the output\n","    char_ind = torch.max(prob, dim=0)[1].item()\n","\n","    return int2char[char_ind], hidden"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"tqtlKwcAiFKb","executionInfo":{"status":"ok","timestamp":1637888250146,"user_tz":180,"elapsed":62,"user":{"displayName":"Marcos Uriel Maillot","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16876029369473785241"}}},"source":["def sample(model, out_len, start='hey'):\n","    model.eval() # eval mode\n","    start = start.lower()\n","    # First off, run through the starting characters\n","    chars = [ch for ch in start]\n","    size = out_len - len(chars)\n","    # Now pass in the previous characters and get a new one\n","    for ii in range(size):\n","        char, h = predict(model, chars)\n","        chars.append(char)\n","\n","    return ''.join(chars)"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OXGoEhB2cZC0","executionInfo":{"status":"ok","timestamp":1637888250148,"user_tz":180,"elapsed":57,"user":{"displayName":"Marcos Uriel Maillot","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16876029369473785241"}},"outputId":"e48b486d-a938-4d2b-d3e2-b2e4e6ed5826"},"source":["text"],"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['hola que tal                     ',\n"," 'como te llamas?                  ',\n"," 'queres comer algo?               ',\n"," 'me llamo marcos                  ',\n"," 'este mensaje fue adivinado       ',\n"," 'este mensajo esta mal escrito    ',\n"," 'tengo que completar algo         ',\n"," 'buenos dias                      ',\n"," 'feliz domingo                    ',\n"," 'al final todo se resuelve        ',\n"," 'otro mensaje mas                 ',\n"," 'espero que funcione              ',\n"," 'siempre que llovio paro          ',\n"," 'pajaro que comio volo            ',\n"," 'el habito no hace al monje       ',\n"," 'la vida sin musica seria un error']"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"7KDpJdmGiFKc","executionInfo":{"status":"ok","timestamp":1637888547787,"user_tz":180,"elapsed":299,"user":{"displayName":"Marcos Uriel Maillot","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16876029369473785241"}},"outputId":"5cd5a918-2a92-4e58-aaf0-b7bb2e31f2a4"},"source":["sample(model, maxlen, 'a  ')"],"execution_count":28,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'a  finomino                      '"]},"metadata":{},"execution_count":28}]},{"cell_type":"markdown","metadata":{"id":"_rXz2PRNiFKd"},"source":["As we can see, the model is able to come up with the sentence â€˜good i am fine â€˜ if we feed it with the words â€˜goodâ€™, achieving what we intended for it to do!"]}]}